{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pravinbachute/Deep-Learning-and-NLP/blob/main/gradient_descent_(batch_).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-1x_ibvAChP",
        "outputId": "5cdad66c-4955-4b40-910c-db9b2d1b823a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss = 41.0000, w1 = 0.3500, w2 = 0.1200\n",
            "Epoch 100: Loss = 0.0075, w1 = 2.0720, w2 = 0.7883\n",
            "Epoch 200: Loss = 0.0041, w1 = 2.0534, w2 = 0.8431\n",
            "Epoch 300: Loss = 0.0023, w1 = 2.0395, w2 = 0.8838\n",
            "Epoch 400: Loss = 0.0012, w1 = 2.0293, w2 = 0.9139\n",
            "Epoch 500: Loss = 0.0007, w1 = 2.0217, w2 = 0.9362\n",
            "Epoch 600: Loss = 0.0004, w1 = 2.0161, w2 = 0.9527\n",
            "Epoch 700: Loss = 0.0002, w1 = 2.0119, w2 = 0.9650\n",
            "Epoch 800: Loss = 0.0001, w1 = 2.0088, w2 = 0.9740\n",
            "Epoch 900: Loss = 0.0001, w1 = 2.0065, w2 = 0.9808\n",
            "\n",
            "Final Weights:\n",
            "w1 = 2.0049, w2 = 0.9857\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Initialize parameters\n",
        "w1 = 0.0\n",
        "w2 = 0.0\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "# Synthetic values (simulate small dataset)\n",
        "x_values = np.array([1, 2, 3, 4])\n",
        "y_values = np.array([3, 5, 7, 9])  # y = 2x + 1 (ground truth)\n",
        "\n",
        "# Batch Gradient Descent\n",
        "for epoch in range(epochs):\n",
        "    # Predictions\n",
        "    y_pred = w1 * x_values + w2\n",
        "\n",
        "    # Compute gradients (mean over all data)\n",
        "    error = y_pred - y_values\n",
        "    dw1 = (2 / len(x_values)) * np.dot(error, x_values)\n",
        "    dw2 = (2 / len(x_values)) * np.sum(error)\n",
        "\n",
        "    # Update weights\n",
        "    w1 -= learning_rate * dw1\n",
        "    w2 -= learning_rate * dw2\n",
        "\n",
        "    # Compute loss\n",
        "    loss = np.mean(error ** 2)\n",
        "\n",
        "    # Print loss every 100 epochs\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}: Loss = {loss:.4f}, w1 = {w1:.4f}, w2 = {w2:.4f}\")\n",
        "\n",
        "print(\"\\nFinal Weights:\")\n",
        "print(f\"w1 = {w1:.4f}, w2 = {w2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stochastic Gradient Descent (SGD) with simple values and 10 epochs\n",
        "\n",
        "# Step 1: Initialize weight and bias\n",
        "w = 0.0     # weight\n",
        "b = 0.0     # bias\n",
        "\n",
        "# Step 2: Set learning rate\n",
        "lr = 0.01\n",
        "\n",
        "# Step 3: Create small synthetic dataset (x, y)\n",
        "# True relationship: y = 2x + 1\n",
        "x = [1, 2, 3, 4]\n",
        "y = [3, 5, 7, 9]\n",
        "\n",
        "# Step 4: Training for 10 epochs\n",
        "for epoch in range(10):\n",
        "    total_error = 0\n",
        "\n",
        "    # SGD â€” update for each individual data point\n",
        "    for i in range(len(x)):\n",
        "        y_pred = w * x[i] + b         # predicted value\n",
        "        error = y_pred - y[i]         # prediction error\n",
        "\n",
        "        # Gradients (for one sample only)\n",
        "        dw = error * x[i]\n",
        "        db = error\n",
        "\n",
        "        # Update weights immediately (SGD style)\n",
        "        w = w - lr * dw\n",
        "        b = b - lr * db\n",
        "\n",
        "        # Accumulate error for monitoring\n",
        "        total_error += error ** 2\n",
        "\n",
        "    # Step 5: Print results after each epoch\n",
        "    print(f\"Epoch {epoch+1}: Loss = {total_error/len(x):.4f}, w = {w:.4f}, b = {b:.4f}\")\n"
      ],
      "metadata": {
        "id": "J-GAzVkSA7Vv",
        "outputId": "4d60eb25-6ad0-48a7-bce9-367824c8b2ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss = 33.2401, w = 0.6266, b = 0.2200\n",
            "Epoch 2: Loss = 16.2972, w = 1.0649, b = 0.3745\n",
            "Epoch 3: Loss = 7.9966, w = 1.3715, b = 0.4831\n",
            "Epoch 4: Loss = 3.9290, w = 1.5858, b = 0.5596\n",
            "Epoch 5: Loss = 1.9350, w = 1.7357, b = 0.6135\n",
            "Epoch 6: Loss = 0.9571, w = 1.8404, b = 0.6518\n",
            "Epoch 7: Loss = 0.4771, w = 1.9135, b = 0.6791\n",
            "Epoch 8: Loss = 0.2413, w = 1.9645, b = 0.6986\n",
            "Epoch 9: Loss = 0.1252, w = 2.0000, b = 0.7127\n",
            "Epoch 10: Loss = 0.0680, w = 2.0247, b = 0.7231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch Gradient Descent (with simple values and only 10 epochs)\n",
        "\n",
        "# Step 1: Initialize weight and bias\n",
        "w = 0.0     # weight\n",
        "b = 0.0     # bias\n",
        "\n",
        "# Step 2: Set learning rate\n",
        "lr = 0.01\n",
        "\n",
        "# Step 3: Create small synthetic dataset (x, y)\n",
        "# Let's say the true relation is: y = 2x + 1\n",
        "x = [1, 2, 3, 4]\n",
        "y = [3, 5, 7, 9]\n",
        "\n",
        "# Step 4: Training for 10 epochs\n",
        "for epoch in range(10):\n",
        "    total_error = 0\n",
        "    dw = 0  # gradient for w\n",
        "    db = 0  # gradient for b\n",
        "\n",
        "    # Loop through all data (batch gradient descent)\n",
        "    for i in range(len(x)):\n",
        "        y_pred = w * x[i] + b         # predicted value\n",
        "        error = y_pred - y[i]         # prediction error\n",
        "        dw += error * x[i]            # accumulate gradient for w\n",
        "        db += error                   # accumulate gradient for b\n",
        "        total_error += error ** 2     # accumulate squared error\n",
        "\n",
        "    # Take average gradients\n",
        "    dw = dw / len(x)\n",
        "    db = db / len(x)\n",
        "\n",
        "    # Step 5: Update weights\n",
        "    w = w - lr * dw\n",
        "    b = b - lr * db\n",
        "\n",
        "    # Step 6: Print results\n",
        "    print(f\"Epoch {epoch+1}: Loss = {total_error/len(x):.4f}, w = {w:.4f}, b = {b:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XjIo7BgAku5",
        "outputId": "08b60f2f-24d1-4054-c720-1de1ee2d7015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss = 41.0000, w = 0.1750, b = 0.0600\n",
            "Epoch 2: Loss = 34.4408, w = 0.3354, b = 0.1150\n",
            "Epoch 3: Loss = 28.9313, w = 0.4823, b = 0.1655\n",
            "Epoch 4: Loss = 24.3034, w = 0.6170, b = 0.2118\n",
            "Epoch 5: Loss = 20.4162, w = 0.7405, b = 0.2542\n",
            "Epoch 6: Loss = 17.1511, w = 0.8536, b = 0.2932\n",
            "Epoch 7: Loss = 14.4084, w = 0.9572, b = 0.3289\n",
            "Epoch 8: Loss = 12.1047, w = 1.0522, b = 0.3617\n",
            "Epoch 9: Loss = 10.1697, w = 1.1393, b = 0.3918\n",
            "Epoch 10: Loss = 8.5443, w = 1.2190, b = 0.4194\n"
          ]
        }
      ]
    }
  ]
}